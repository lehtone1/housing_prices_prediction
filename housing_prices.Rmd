---
title: "Housing Prices Prediction"
author: "Eero Lehtonen"
date: "7/4/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Summary

In this project the task was to predict the sell price of a house from the house's features. 

Approach:

1. Loading dataset
2. Exploring dataset structure and features
3. Removing columns used for book keeping and with a lot of na-values
4. Converting factor columns to be of type factors
5. Data imputation (numeric -> median, factor -> most common)
6. Creating dummy variables
7. Feature Selection (Removing near zero variance predictors and highly correlated features)
8. Creating Baseline prediction with average sell price
9. Training and predicting with linear regression

Packages Used:

dplyr, caret

Predictors:

- House features i.e. size, number of bathrooms, garage, pool etc.

Outcome

- Sell price of the house

## Background

Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.

With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.

(From Kaggle challenge)


## Loading Data

```{r}
library(dplyr)
library(caret)
library(ggplot2)
library(reshape2)
```

```{r}
train <- read.csv("data/train.csv")
test <- read.csv("data/test.csv")

ids <- test$Id
train_y <- data.frame(SalePrice = train$SalePrice)
train <- select(train, -SalePrice)
features <- rbind(train, test)
```

## Understanding dataset

```{r}
str(train)
```

```{r}
colSums(is.na(train)) / dim(train)[1]
```

```{r}
colSums(train == "") / dim(train)[1]
```
## Visualizing dataset

```{r}
classes <- lapply(features, class)

factor_columns <- names(classes[classes == "factor"])
integer_columns <- names(classes[classes == "integer"])
```

### Factorial data

```{r}
melt_features_factors <- melt(features[factor_columns], id.vars= NULL, factorsAsStrings=TRUE, na.rm = TRUE)
```

```{r}
ggplot(data=melt_features_factors, aes(x = value)) + stat_count() + facet_wrap(~variable, scales = "free")
```

### Integer data

```{r}
melt_features_integers <- melt(features[integer_columns], factorsAsStrings=TRUE, na.rm = TRUE)
```

```{r}
ggplot(data=melt_features_integers, aes(x = value)) + stat_density() + facet_wrap(~variable, scales = "free", , ncol = 6)
```

```{r}
ggplot(data=melt_features_integers, aes(x = variable, y = value)) + stat_boxplot() + facet_wrap(~variable, scales = "free")
```




# Feature Engineering

```{r}

features <- features %>%
  mutate(
    TotalSF = TotalBsmtSF + X1stFlrSF + X2ndFlrSF,
    TotalBath = BsmtFullBath + BsmtHalfBath + FullBath + HalfBath,
    PorchSF = WoodDeckSF + OpenPorchSF + EnclosedPorch + X3SsnPorch + ScreenPorch,
    YrBuiltOrRemod = pmax(YearBuilt, YearRemodAdd),
    HasPool = PoolArea > 0,
    HasFireplace = Fireplaces > 0,
    HasGarage = !is.na(GarageType),
    HasBasement = TotalBsmtSF > 0,
    Has2ndFloor = X2ndFlrSF > 0 
  )

```
## Cleaning dataset

Removing unusable columns

```{r}
train <- select(train, -Id, -PoolQC, -Fence, -MiscFeature, -Alley)

test <- select(test, -Id, -PoolQC, -Fence, -MiscFeature, -Alley)


```

Factor Conversion


```{r}

features$MSSubClass <- as.factor(features$MSSubClass)
features$OverallQual <- as.factor(features$OverallQual)
features$OverallCond <- as.factor(features$OverallCond)
features$BsmtFullBath <- as.factor(features$BsmtFullBath)
features$BsmtHalfBath <- as.factor(features$BsmtHalfBath)
features$FullBath <- as.factor(features$FullBath)
features$HalfBath <- as.factor(features$HalfBath)
features$BedroomAbvGr <- as.factor(features$BedroomAbvGr)
features$KitchenAbvGr <- as.factor(features$KitchenAbvGr)
features$TotRmsAbvGrd <- as.factor(features$TotRmsAbvGrd)
features$GarageCars <- as.factor(features$GarageCars)
features$MoSold <- as.factor(features$MoSold)
features$HasPool <- as.factor(features$HasPool)
features$HasFireplace <- as.factor(features$HasFireplace)
features$HasGarage <- as.factor(features$HasGarage)
features$HasBasement <- as.factor(features$HasBasement)
features$Has2ndFloor <- as.factor(features$Has2ndFloor)

```

Data imputation

Get classes for each column

```{r}
classes <- lapply(features, class)
```

In factor columns, replace the NA-values with the most frequent value 

```{r}
factor_columns <- names(classes[classes == "factor"])

impute_factor_column <- function(column) {
  column[is.na(column)] <- names(which.max(table(column)))
  column
}

features[factor_columns] <- lapply(features[factor_columns], impute_factor_column)

```

In integer columns, replace the NA-values with the median value.

```{r}
integer_columns <- names(classes[classes == "integer"])

impute_integer_column <- function(column) {
  column[is.na(column)] <- median(column, na.rm = TRUE)
  column
}

features[integer_columns] <- lapply(features[integer_columns], impute_integer_column)

```

Dummy variables

```{r}

dummies <- dummyVars(~., data = features)
features <- data.frame(predict(dummies, newdata = features))

```

## Feature Selection

Remove features with zero variance

```{r}
near_zero_var_columns <- nearZeroVar(features, freqCut = 99, uniqueCut = 1)
features <- select(features, -near_zero_var_columns)
```

Remove highly correlated features

```{r}
correlationMatrix <- cor(features)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=.85)
highlyCorrelated

features <- select(features, -highlyCorrelated)
```

Separate train and test set
```{r}
train_x <- features[1:nrow(train_y),]
test_x <- features[(nrow(train_y)+1):nrow(features),]

train <- cbind(train_x, train_y)
```

## Prediction

### Baseline

```{r}
baseline <- mean(train_y$SalePrice)
pred <- rep(baseline, length(ids))
```

Write submission file

```{r}
submission_data <- data.frame(Id = ids, SalePrice = pred)
write.csv(submission_data, file="data/submission_baseline.csv", row.names=FALSE)
```
# Models to try

### Model Selection

Train model

```{r}
# library(parallel)
# library(doParallel)
# 
# cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
# registerDoParallel(cluster)
# 
# control <- trainControl(method="repeatedcv", number=5, allowParallel = TRUE)
# 
# model_linear <- train(SalePrice ~ ., data=train, method="lm", trControl=control)
# model_ridge <- train(SalePrice ~ ., data=train, method="ridge", trControl=control)
# model_lasso <- train(SalePrice ~ ., data=train, method="lasso", trControl=control)
# model_elastic <- train(SalePrice ~ ., data=train, method="enet", trControl=control)
# 
# saveRDS(model_linear, "model_linear.rds")
# saveRDS(model_ridge, "model_ridge.rds")
# saveRDS(model_lasso, "model_lasso.rds")
# saveRDS(model_elastic, "model_elastic.rds")
# 
# stopCluster(cluster)
# registerDoSEQ()
```

Compare models

```{r}
model_linear <-readRDS("model_linear.rds")
model_ridge <-readRDS("model_ridge.rds")
model_lasso <-readRDS("model_lasso.rds")
model_elastic <-readRDS("model_elastic.rds")

results <- resamples(list(linear=model_linear, ridge=model_ridge, lasso=model_lasso, enet=model_elastic))
summary(results)
```

Make Precitions

```{r}
pred2 <- predict(model_linear, test_x)
pred3 <- predict(model_ridge, test_x)
pred4 <- predict(model_lasso, test_x)
pred5 <- predict(model_elastic, test_x)
```

Write submission file

```{r}
submission_data <- data.frame(Id = ids, SalePrice = pred2)
write.csv(submission_data, file="data/submission_linear.csv", row.names=FALSE)

submission_data <- data.frame(Id = ids, SalePrice = pred3)
write.csv(submission_data, file="data/submission_ridge.csv", row.names=FALSE)

submission_data <- data.frame(Id = ids, SalePrice = pred4)
write.csv(submission_data, file="data/submission_lasso.csv", row.names=FALSE)

submission_data <- data.frame(Id = ids, SalePrice = pred5)
write.csv(submission_data, file="data/submission_enet.csv", row.names=FALSE)
```

## Conclusion

Final competition score with ENET: 0.14842
Position: 3056



