---
title: "Housing Prices Prediction"
author: "Eero Lehtonen"
date: "7/4/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Summary

In this project the task was to predict the sell price of a house from the house's features. 

Approach:

1. Loading dataset
2. Exploring dataset structure and features
3. Removing columns used for book keeping and with a lot of na-values
4. Converting factor columns to be of type factors
5. Data imputation (numeric -> median, factor -> most common)
6. Creating dummy variables
7. Feature Selection (Removing near zero variance predictors and highly correlated features)
8. Creating Baseline prediction with average sell price
9. Training and predicting with linear regression

Packages Used:

dplyr, caret

Predictors:

- House features i.e. size, number of bathrooms, garage, pool etc.

Outcome

- Sell price of the house

## Background

Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.

With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.

(From Kaggle challenge)


## Loading Data

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(caret)
library(ggplot2)
library(reshape2)
library(corrplot)
library(moments)
library(tidyr)
library(patchwork)
```

We start the analysis by loading the data.
```{r}
train <- read.csv("data/train.csv")
test <- read.csv("data/test.csv")
```

In the training dataset we separate the predictors from the outcome. We then create a variable features from the predictors in training and testing set so that we don't have to repeat data manipulation separately for training and testing set. We also separate the id-column from the testing set, as it is needed for the submission file.


```{r}
train_y <- data.frame(SalePrice = train$SalePrice)
train_x <- select(train, -SalePrice)
features <- rbind(train_x, test)
ids <- test$Id
```

## Correcting Variable Type

From reading the data description file and comparing it to the dataset column types, we noticed that there are several columns that are of numerical type that should be factors. We convert these columns to be of correct type.

```{r}
features$MSSubClass <- as.factor(features$MSSubClass)
features$OverallQual <- as.factor(features$OverallQual)
features$OverallCond <- as.factor(features$OverallCond)
features$BsmtFullBath <- as.factor(features$BsmtFullBath)
features$BsmtHalfBath <- as.factor(features$BsmtHalfBath)
features$FullBath <- as.factor(features$FullBath)
features$HalfBath <- as.factor(features$HalfBath)
features$BedroomAbvGr <- as.factor(features$BedroomAbvGr)
features$KitchenAbvGr <- as.factor(features$KitchenAbvGr)
features$TotRmsAbvGrd <- as.factor(features$TotRmsAbvGrd)
features$GarageCars <- as.factor(features$GarageCars)

train$OverallQual <- as.factor(train$OverallQual)
train$GarageCars <- as.factor(train$GarageCars)
```


## Understanding dataset

### Outcome Variable

We start the analysis by looking at the dimensions of the training-set. 

```{r}
dim(train)
```

We plot the outcome variable SalePrice. It seems that the outcome variable is quite closely, but not exactly normally distributed. 

```{r, echo=FALSE}
ggplot(data = train_y, aes(x=SalePrice)) + geom_histogram()
```

We verify this by measuring skewness and kurtosis.

```{r}
print(paste("Skewness:", skewness(train_y$SalePrice)))
print(paste("Kurtosis:", kurtosis(train_y$SalePrice)))
```

### Predictors

Beforehand we estimated that important features that could have a significant effect on SalePrice could be Neighborhood, OverallQual, GrLivArea, GarageCars, BsmtSF and YearBuilt. We visualize all the relationships between these variables and SalePrice.

The above gound living area shows almost a linear relationship with salesprice. This definitely is an important feature. From the graph we can also see that with very large above ground living area there seems to be some outliers that should taken into closer inspection

```{r, echo=FALSE}
ggplot(data=train, aes(x=GrLivArea, y=SalePrice)) + geom_point()
```

Year built and seems to also have a slight linear relationship with sale's price. 

```{r, echo=FALSE}
ggplot(data=train, aes(x=YearBuilt, y=SalePrice)) + geom_point()
```

Total basement square feet has a very similar relatinship than above ground living area, which makes sense as basement size should go quite well hand in hand with above ground living area size.

```{r, echo=FALSE}
ggplot(data=train, aes(x=TotalBsmtSF, y=SalePrice)) + geom_point()
```

The neighborhood seems to have some effect to price, but the effect is smaller than we would expect.

```{r, echo=FALSE}
ggplot(data=train, aes(x=Neighborhood, y=SalePrice)) + geom_boxplot()
```

The overall quality has also very clear relationship with sales price. It is tho unknown how overall quality is calculated.

```{r, echo=FALSE}
ggplot(data=train, aes(x=OverallQual, y=SalePrice)) + geom_boxplot()
```

Garage cars seem to affect the sale's price, but odly having four car spaces in carage the apartment sale prices are smaller than with three spaces.

```{r, echo=FALSE}
ggplot(data=train, aes(x=GarageCars, y=SalePrice)) + geom_boxplot()
```

## Cleaning dataset

### NA-columns

We start by looking at the percentage of the NA-values in each columns.

```{r, warning=FALSE}
features %>% 
  summarize_all(funs(sum(is.na(.)) / length(.))) %>% 
  gather() %>% 
  arrange(-value) %>% 
  head(35)
```

There seems to be pretty much NA values in some of the columns. When we look at description of the data it infact shows that for many columns NA does not mean that the data is actually missing, but instead that the feature does not exist.

Columns where NA-values mean that the feature does not exist:

Alley, Bsmt..., FireplaceQu, Garage..., PoolQC  Fence, MiscFeatures

In these cases it might be best to just change the NA-value for something else i.e. no, so that the information will not disapear. 

Other columns containing NA-values: 

LotFrontage, MasVnrType, MasVnrArea, MSZoning, Utilities, BsmtFullBath, BsmtHalfBath, Functional, Exterior1st, Exterior2nd, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF , Electrical, KitchenQual, GarageCars, GarageArea, SaleType

### Removing Unusable columns

Removing Id column that is used for book keeping

```{r}
train <- select(train, -Id)
test <- select(test, -Id)
```

### Imputation

Function for mode, used for factor column data imputation

```{r}
mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
```

#### LotFrontage and MSZoning

For LotFrontage and MSZoning we are going to use a special imputation technique. For both of the columns we will group the columns by another column and replace the na-valeus based on the mode of that grouping.

In LotFrontage column we are going to use Neighborhood column, as we would think that in the same neigborhood there would be similar distances from the property to the street.

```{r}
features$LotFrontage <- with(features, ave(LotFrontage, Neighborhood, FUN = function(i) replace(i, is.na(i), median(i, na.rm=TRUE))))
```

With MSZoning case we will do the imputation based on MSSubClass-column. As we would believe that the type of dwelling and the general zoning classification would be related. 

```{r}
features$MSZoning <- with(features, ave(MSZoning, MSSubClass, FUN = function(i) replace(i, is.na(i), mode(i))))
```

#### Missing features

With the columns where NA-values meant that the feature was missing we will replace the NA-values with a new factor level "No".

```{r}
classes <- lapply(features, class)
factor_columns <- names(classes[classes == "factor"])

impute_no <- function(column) {
  levels(column) <- c(levels(column),"No")
  column[is.na(column)] <- "No"
  column
}

missing_features <- grep("^Gar|Bsmt|Alley|Fireplace|Pool|Fence|Misc", factor_columns, value=TRUE)

features[missing_features] <- lapply(features[missing_features], impute_no)


```


#### Mode and Median imputation

For the rest of the columns with NA-values we will replace the missing values with mode or meadian depending if the column is of factor or numerical type.

```{r}
classes <- lapply(features, class)
```

In factor columns, replace the NA-values with the mode.

```{r}
factor_columns <- names(classes[classes == "factor"])

impute_factor_column <- function(column) {
  column[is.na(column)] <- names(which.max(table(column)))
  column
}

features[factor_columns] <- lapply(features[factor_columns], impute_factor_column)

```

In integer columns, replace the NA-values with the median value.

```{r}
integer_columns <- names(classes[classes == "integer"])

impute_integer_column <- function(column) {
  column[is.na(column)] <- median(column, na.rm = TRUE)
  column
}

features[integer_columns] <- lapply(features[integer_columns], impute_integer_column)
```
### Target Variable

```{r, echo=FALSE}
g1 <- ggplot(train_y, aes(x=SalePrice)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ 
  labs(x = "", y = "")

g2 <- ggplot(train_y, aes(y=SalePrice)) + 
 geom_boxplot(aes(x=""), colour="black", fill="white")+
  coord_flip()+ 
  labs(x = "", y = "")

g3 <- ggplot(train_y, aes(sample = SalePrice))+ 
  stat_qq()+
  stat_qq_line()+ 
  labs(x = "", y = "")
```

From the plots we can see that the outputvariable is skewed to the right. 

```{r, echo=FALSE, messages=FALSE}
g3 | g1 / g2 
```

We fix this by performing a log transform to the output variable.

```{r}
train_y <- train_y %>% mutate(SalePrice = log(SalePrice))
```

```{r, echo=FALSE}
g1 <- ggplot(train_y, aes(x=SalePrice)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ 
  labs(x = "", y = "")

g2 <- ggplot(train_y, aes(y=SalePrice)) + 
 geom_boxplot(aes(x=""), colour="black", fill="white")+
  coord_flip()+ 
  labs(x = "", y = "")

g3 <- ggplot(train_y, aes(sample = SalePrice))+ 
  stat_qq()+
  stat_qq_line()+ 
  labs(x = "", y = "")
```

We verify that the output variable is now aproximately normally distributed.

```{r, echo=FALSE, messages=FALSE}
g3 | g1 / g2 
```

### Dummy variables

For computational reasons we will create dummy variables (one hot encode) for the factor variables.

```{r}
dummies <- dummyVars(~., data = features)
features <- data.frame(predict(dummies, newdata = features))
```

## Feature Selection

We will not go too deeply on feature selection in this markdown-file. The feature selection methods used were removing zero variance and very near to zero variance predictors and removing features that were highly correlated with each other.

Remove features with zero variance

```{r}
near_zero_var_columns <- nearZeroVar(features, freqCut = 99, uniqueCut = 1)
names(features)[near_zero_var_columns]
features <- select(features, -near_zero_var_columns)
```

Remove highly correlated features

```{r}
correlationMatrix <- cor(features)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=.85)
names(features)[highlyCorrelated]

features <- select(features, -highlyCorrelated)
```

## Prediction

Separate train and test set

```{r}
train_x <- features[1:nrow(train_y),]
test_x <- features[(nrow(train_y)+1):nrow(features),]

train <- cbind(train_x, train_y)
```

### Baseline

As the baseline prediction we will use the mean sale price.

```{r}
baseline <- mean(train_y$SalePrice)
pred <- rep(baseline, length(ids))
```

Write submission file

```{r}
submission_data <- data.frame(Id = ids, SalePrice = pred)
write.csv(submission_data, file="data/submission_baseline.csv", row.names=FALSE)
```

### Model Selection

Train model

```{r}
# library(parallel)
# library(doParallel)
# 
# cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
# registerDoParallel(cluster)
# 
# control <- trainControl(method="repeatedcv", number=5, allowParallel = TRUE)
# 
# model_linear <- train(SalePrice ~ ., data=train, method="lm", trControl=control)
# model_ridge <- train(SalePrice ~ ., data=train, method="ridge", trControl=control)
# model_lasso <- train(SalePrice ~ ., data=train, method="lasso", trControl=control)
# model_elastic <- train(SalePrice ~ ., data=train, method="enet", trControl=control)
# 
# saveRDS(model_linear, "model_linear.rds")
# saveRDS(model_ridge, "model_ridge.rds")
# saveRDS(model_lasso, "model_lasso.rds")
# saveRDS(model_elastic, "model_elastic.rds")
# 
# stopCluster(cluster)
# registerDoSEQ()
```

Compare models. The Lasso model ended being the most accurate in the final version.

```{r}
model_linear <-readRDS("model_linear.rds")
model_ridge <-readRDS("model_ridge.rds")
model_lasso <-readRDS("model_lasso.rds")
model_elastic <-readRDS("model_elastic.rds")

results <- resamples(list(linear=model_linear, ridge=model_ridge, lasso=model_lasso, enet=model_elastic))
summary(results)
```

Make Precitions

```{r}
pred2 <- exp(predict(model_linear, test_x))
pred3 <- exp(predict(model_ridge, test_x))
pred4 <- exp(predict(model_lasso, test_x))
pred5 <- exp(predict(model_elastic, test_x))
```

Write submission file

```{r}
submission_data <- data.frame(Id = ids, SalePrice = pred2)
write.csv(submission_data, file="data/submission_linear.csv", row.names=FALSE)

submission_data <- data.frame(Id = ids, SalePrice = pred3)
write.csv(submission_data, file="data/submission_ridge.csv", row.names=FALSE)

submission_data <- data.frame(Id = ids, SalePrice = pred4)
write.csv(submission_data, file="data/submission_lasso.csv", row.names=FALSE)

submission_data <- data.frame(Id = ids, SalePrice = pred5)
write.csv(submission_data, file="data/submission_enet.csv", row.names=FALSE)
```

## Conclusion

Final competition score with ENET: 0.13393
Position: 1881/5037



