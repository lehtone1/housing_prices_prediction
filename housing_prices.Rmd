---
title: "Housing Prices Prediction"
author: "Eero Lehtonen"
date: "7/4/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Summary

In this project the task was to predict the sell price of a house from the house's features. 

Approach:

1. Loading dataset
2. Correcting Variable Types
3. Understanding Dataset
    1. Outcome Variable
	  2. Predictors
4. Cleaning dataset 
    1. Removing Book Keeping Columns
    2. Imputation
    3. Outcome Variable Skewness
    4. Creating Dummy Variables
5. Feature Selection 
    1. Near Zero Variance Columns
    2. Highly Correlated Features
6. Prediction
    1. Creating a Baseline
    2. Model comparasion
    3. Creating a submission file
7. Conclusion

Packages Used:

dplyr, caret, ggplot2, reshape2, moments, tidyr, patchwork

Predictors:

- House features i.e. size, number of bathrooms, garage, pool etc.

Outcome

- Sell price of the house

## Background

Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.

With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.

(From Kaggle challenge)


## Loading Dataset

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(caret)
library(ggplot2)
library(reshape2)
library(moments)
library(tidyr)
library(patchwork)
```

We start the analysis by loading the data.
```{r}
train <- read.csv("data/train.csv")
test <- read.csv("data/test.csv")
```

In the training dataset we separate the predictors from the outcome. We then create a new variable "features" from the predictors in the training and in the testing set so that we don't have to repeat data manipulation separately for the training and the testing set. We also separate the id-column from the testing set, as it is needed for the submission file.


```{r}
train_y <- data.frame(SalePrice = train$SalePrice)
train_x <- select(train, -SalePrice)
features <- rbind(train_x, test)
ids <- test$Id
```

## Correcting Variable Types

From reading the data description file and comparing it to the dataset column types, we noticed that there are several columns that are of numerical type that should be factors. We convert these columns to be of correct type.

```{r}
features$MSSubClass <- as.factor(features$MSSubClass)
features$OverallQual <- as.factor(features$OverallQual)
features$OverallCond <- as.factor(features$OverallCond)
features$BsmtFullBath <- as.factor(features$BsmtFullBath)
features$BsmtHalfBath <- as.factor(features$BsmtHalfBath)
features$FullBath <- as.factor(features$FullBath)
features$HalfBath <- as.factor(features$HalfBath)
features$BedroomAbvGr <- as.factor(features$BedroomAbvGr)
features$KitchenAbvGr <- as.factor(features$KitchenAbvGr)
features$TotRmsAbvGrd <- as.factor(features$TotRmsAbvGrd)
features$GarageCars <- as.factor(features$GarageCars)

# For visualizing purposes
train$OverallQual <- as.factor(train$OverallQual)
train$GarageCars <- as.factor(train$GarageCars)
```


## Understanding Dataset

### Outcome Variable

We start the analysis by looking at the dimensions of the training-set. 

```{r}
dim(train)
```

We plot the outcome variable SalePrice. It seems that the outcome variable is quite closely, but not exactly normally distributed. 

```{r, echo=FALSE}
ggplot(data = train_y, aes(x=SalePrice)) + geom_histogram()
```

We verify this by measuring skewness and kurtosis.

```{r}
print(paste("Skewness:", skewness(train_y$SalePrice)))
print(paste("Kurtosis:", kurtosis(train_y$SalePrice)))
```

### Predictors

Beforehand we estimated that important features that could have a significant effect on SalePrice could be Neighborhood, OverallQual, GrLivArea, GarageCars, BsmtSF and YearBuilt. We visualize all the relationships between these variables and SalePrice.

The above gound living area shows almost a linear relationship with salesprice. This definitely is an important feature. From the graph we can also see that with very large above ground living area there seems to be some outliers that should be taken into closer inspection

```{r, echo=FALSE}
ggplot(data=train, aes(x=GrLivArea, y=SalePrice)) + geom_point()
```

Year built seems to also have a slight linear relationship with sale price. 

```{r, echo=FALSE}
ggplot(data=train, aes(x=YearBuilt, y=SalePrice)) + geom_point()
```

Total basement square feet has a very similar relatinship than above ground living area, which makes sense as basement size should go quite well hand in hand with above ground living area size.

```{r, echo=FALSE}
ggplot(data=train, aes(x=TotalBsmtSF, y=SalePrice)) + geom_point()
```

The neighborhood seems to have some effect to price, but the effect is smaller than we would expect.

```{r, echo=FALSE}
ggplot(data=train, aes(x=Neighborhood, y=SalePrice)) + geom_boxplot()
```

The overall quality has also very clear relationship with sales price. It is though unknown how overall quality is calculated.

```{r, echo=FALSE}
ggplot(data=train, aes(x=OverallQual, y=SalePrice)) + geom_boxplot()
```

Garage cars seem to affect the sale price, but odly having four car spaces in carage the apartment sale prices are smaller than with three spaces.

```{r, echo=FALSE}
ggplot(data=train, aes(x=GarageCars, y=SalePrice)) + geom_boxplot()
```

## Cleaning dataset

### Removing Book Keeping Columns

Removing Id column that is used for book keeping

```{r}
train <- select(train, -Id)
test <- select(test, -Id)
```

### Imputation

We start by looking at the percentage of the NA-values in each columns.

```{r, warning=FALSE}
features %>% 
  summarize_all(funs(sum(is.na(.)) / length(.))) %>% 
  gather() %>% 
  arrange(-value) %>% 
  head(35)
```

There seems to be pretty much NA values in some of the columns. When we look at description of the data it infact shows that for many columns NA does not mean that the data is actually missing, but instead that the feature does not exist.

Columns where NA-values mean that the feature does not exist:

Alley, Bsmt..., FireplaceQu, Garage..., PoolQC  Fence, MiscFeatures

In these cases it might be best to just change the NA-value for something else i.e. a now factor level "No", so that the information will not disapear. 

Other columns containing NA-values: 

LotFrontage, MasVnrType, MasVnrArea, MSZoning, Utilities, BsmtFullBath, BsmtHalfBath, Functional, Exterior1st, Exterior2nd, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF , Electrical, KitchenQual, GarageCars, GarageArea, SaleType

Function for mode, used for factor column data imputation

```{r}
mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
```

#### LotFrontage and MSZoning

For LotFrontage and MSZoning we are going to use a special imputation technique. For both of the columns we will group the columns by another column and replace the NA-values based on the mode of that grouping.

In LotFrontage column we are going to use Neighborhood column, as we would think that in the same neigborhood there would be similar distances from the property to the street.

```{r}
features$LotFrontage <- with(features, ave(LotFrontage, Neighborhood, FUN = function(i) replace(i, is.na(i), median(i, na.rm=TRUE))))
```

With MSZoning case we will do the imputation based on MSSubClass-column. As we would believe that the type of dwelling and the general zoning classification would be related. 

```{r}
features$MSZoning <- with(features, ave(MSZoning, MSSubClass, FUN = function(i) replace(i, is.na(i), mode(i))))
```

#### Missing features

With the columns where NA-values meant that the feature was missing we will replace the NA-values with a new factor level "No".

```{r}
classes <- lapply(features, class)
factor_columns <- names(classes[classes == "factor"])

impute_no <- function(column) {
  levels(column) <- c(levels(column),"No")
  column[is.na(column)] <- "No"
  column
}

missing_features <- grep("^Gar|Bsmt|Alley|Fireplace|Pool|Fence|Misc", factor_columns, value=TRUE)

features[missing_features] <- lapply(features[missing_features], impute_no)


```


#### Mode and Median imputation

For the rest of the columns with NA-values we will replace the missing values with mode or meadian depending if the column is of factor or numerical type.

```{r}
classes <- lapply(features, class)
```

In factor columns, replace the NA-values with the mode.

```{r}
factor_columns <- names(classes[classes == "factor"])

impute_factor_column <- function(column) {
  column[is.na(column)] <- names(which.max(table(column)))
  column
}

features[factor_columns] <- lapply(features[factor_columns], impute_factor_column)

```

In integer columns, replace the NA-values with the median value.

```{r}
integer_columns <- names(classes[classes == "integer"])

impute_integer_column <- function(column) {
  column[is.na(column)] <- median(column, na.rm = TRUE)
  column
}

features[integer_columns] <- lapply(features[integer_columns], impute_integer_column)
```
### Outcome Variable Skewness

```{r, echo=FALSE}
g1 <- ggplot(train_y, aes(x=SalePrice)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ 
  labs(x = "", y = "")

g2 <- ggplot(train_y, aes(y=SalePrice)) + 
 geom_boxplot(aes(x=""), colour="black", fill="white")+
  coord_flip()+ 
  labs(x = "", y = "")

g3 <- ggplot(train_y, aes(sample = SalePrice))+ 
  stat_qq()+
  stat_qq_line()+ 
  labs(x = "", y = "")
```

From the plots we can see that the outputvariable is skewed to the right. 

```{r, echo=FALSE, messages=FALSE}
g3 | g1 / g2 
```

We fix this by performing a log transform to the output variable.

```{r}
train_y <- train_y %>% mutate(SalePrice = log(SalePrice))
```

```{r, echo=FALSE}
g1 <- ggplot(train_y, aes(x=SalePrice)) + 
 geom_histogram(aes(y=..density..), colour="black", fill="white")+
 geom_density(alpha=.2, fill="#FF6666")+ 
  labs(x = "", y = "")

g2 <- ggplot(train_y, aes(y=SalePrice)) + 
 geom_boxplot(aes(x=""), colour="black", fill="white")+
  coord_flip()+ 
  labs(x = "", y = "")

g3 <- ggplot(train_y, aes(sample = SalePrice))+ 
  stat_qq()+
  stat_qq_line()+ 
  labs(x = "", y = "")
```

We verify that the output variable is now aproximately normally distributed.

```{r, echo=FALSE, messages=FALSE}
g3 | g1 / g2 
```

### Creating Dummy Variables

For computational reasons we will create dummy variables (one hot encoding) for the factor variables.

```{r}
dummies <- dummyVars(~., data = features)
features <- data.frame(predict(dummies, newdata = features))
```

## Feature Selection

We will not go too deeply on feature selection in this markdown-file. The feature selection methods used were removing zero variance and very near to zero variance predictors and removing features that were highly correlated with each other.

### Near Zero Variance Columns

Remove features with zero variance or very close to it

```{r}
near_zero_var_columns <- nearZeroVar(features, freqCut = 99, uniqueCut = 1)
names(features)[near_zero_var_columns]
features <- select(features, -near_zero_var_columns)
```

### Highly Correlated Features

Remove features that are very highly correlated with each other

```{r}
correlationMatrix <- cor(features)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=.85)
names(features)[highlyCorrelated]

features <- select(features, -highlyCorrelated)
```

## Prediction

Separate train and test set

```{r}
train_x <- features[1:nrow(train_y),]
test_x <- features[(nrow(train_y)+1):nrow(features),]

train <- cbind(train_x, train_y)
```

### Creating a Baseline

As the baseline prediction we will use the mean sale price.

```{r}
baseline <- mean(train_y$SalePrice)
baseline
pred <- rep(baseline, length(ids))
```

### Model Selection

We will train all together four models the used models are the following:

- Linear 
- Ridgre regression
- Lasso
- Elasticnet

We use parallel processing in the training process to make the training faster.

```{r}
# library(parallel)
# library(doParallel)
# 
# cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
# registerDoParallel(cluster)
# 
# control <- trainControl(method="repeatedcv", number=5, allowParallel = TRUE)
# 
# model_linear <- train(SalePrice ~ ., data=train, method="lm", trControl=control)
# model_ridge <- train(SalePrice ~ ., data=train, method="ridge", trControl=control)
# model_lasso <- train(SalePrice ~ ., data=train, method="lasso", trControl=control)
# model_elastic <- train(SalePrice ~ ., data=train, method="enet", trControl=control)
# 
# saveRDS(model_linear, "model_linear.rds")
# saveRDS(model_ridge, "model_ridge.rds")
# saveRDS(model_lasso, "model_lasso.rds")
# saveRDS(model_elastic, "model_elastic.rds")
# 
# stopCluster(cluster)
# registerDoSEQ()
```

Compare models. The Lasso model ended being the most accurate in the final version.

```{r}
model_linear <-readRDS("model_linear.rds")
model_ridge <-readRDS("model_ridge.rds")
model_lasso <-readRDS("model_lasso.rds")
model_elastic <-readRDS("model_elastic.rds")

results <- resamples(list(linear=model_linear, ridge=model_ridge, lasso=model_lasso, enet=model_elastic))
summary(results)
```

Make Precitions. As we did a log transform for the outcome variable we need to transform the outcome variables back to originals by taking the exponent. 

```{r}
pred <- exp(predict(model_lasso, test_x))
```

Write submission file

```{r}
submission_data <- data.frame(Id = ids, SalePrice = pred)
write.csv(submission_data, file="data/submission_lasso.csv", row.names=FALSE)
```

## Conclusion

In the project I learned a lot about the main idea of the machine learning process. Make the most simple model possible that gives some results and after that try out different things and see if the results improve.

In the project I was able to test different kind of imputation methods that were unfamiliar to me beforehand. I also learned a lot about exploratory analysis, of what are important things to explore before doing an analysis. 

Final competition score with Lasso: 0.13393

Position: 1881/5037



